#!/usr/bin/env python3
#-*- coding: utf-8 -*-

import argparse
import logging
import re
import sys
import time
import yaml

from lib.Fs2EsIndexer import *


parser = argparse.ArgumentParser(description='Indexes the names of files and directories into elasticsearch')

parser.add_argument(
    'action',
    default='index',
    nargs='?',
    help='What do you want to do? "index" (default), "daemon", "search", "clear", "analyze_index", "enable_slowlog" or "disable_slowlog"?'
)

parser.add_argument(
    '--search-term',
    action='store',
    default=None,
    help='Action "search" only: The term we want to search for in the index'
)

parser.add_argument(
    '--search-filename',
    action='store',
    default=None,
    help='Action "search" only: The filename we want to search for in the index'
)

parser.add_argument(
    '--search-path',
    action='store',
    help='Action "search" only: The server(!) path we want to search in (use the samba share\'s "path")'
)

parser.add_argument(
    '--config',
    action='store',
    dest='configFile',
    default='/etc/fs2es-indexer/config.yml',
    help='The configuration file to be read'
)

parser.add_argument(
    '--verbose',
    '-v',
    action='store_true',
    dest='verbose',
    default=False,
    help='Print more verbose messages'
)

parser.add_argument(
    '--log-level-es',
    action='store',
    dest='logLevelEs',
    default='ERROR',
    help='The logging level of the elasticsearch plugin (DEBUG, INFO, WARN, ERROR, FATAL).'
)

args = parser.parse_args()

logger = logging.getLogger('fs2es-indexer')
logging.basicConfig(
    stream=sys.stdout,
    level=logging.INFO,
    format='%(asctime)s %(name)s %(levelname)s %(message)s'
)
if args.verbose:
    logger.setLevel(logging.DEBUG)

logging.getLogger('elasticsearch').setLevel(args.logLevelEs)
logging.getLogger('elastic_transport').setLevel(args.logLevelEs)

logger.info('Reading config file "%s"' % args.configFile)
with open(args.configFile, 'r') as stream:
    config = yaml.safe_load(stream)

indexer = Fs2EsIndexer(config, logger)

if args.action == 'index':
    logger.info('Starting indexing run...')

    indexer.elasticsearch_prepare_index()
    indexer.elasticsearch_get_all_ids()
    indexer.index_directories()
elif args.action == 'clear':
    indexer.clear_index()
elif args.action == 'daemon':
    indexer.daemon()
elif args.action == 'search':
    if args.search_path is None:
        parser.error('"search" requires --search-path')

    resp = indexer.search(args.search_path, args.search_term, args.search_filename)

    if not args.search_path.endswith('/'):
        args.search_path = args.search_path + '/'

    hits = 0
    for hit in resp['hits']['hits']:
        path = hit['_source']['path']['real']
        if not path.startswith(args.search_path):
            continue

        hits += 1

        if args.verbose:
            logger.info('- "%s": %s' % (hit['_source']['file']['filename'], json.dumps(hit)))
        else:
            logger.info('- "%s"' % hit['_source']['path']['real'])

    logger.info('Found %d elasticsearch documents.' % hits)

elif args.action == 'enable_slowlog':
    indexer.enable_slowlog()
elif args.action == 'disable_slowlog':
    indexer.disable_slowlog()
elif args.action == 'analyze_index':
    if indexer.elasticsearch_analyze_index():
        logger.info('Recreating the elasticsearch index is necessary.')
    else:
        logger.info('Recreating the elasticsearch index is not necessary.')
else:
    logger.info('Unknown action "%s", allowed are "index" (default), "daemon", "search", "clear", "enable_slowlog" or "disable_slowlog".' % args.action)
